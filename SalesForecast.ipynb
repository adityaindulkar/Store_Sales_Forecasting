{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1891c1-68e3-49c2-bc8f-d4f15fbd7c61",
   "metadata": {},
   "source": [
    "IMPORTING REQUIREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b2211-ae86-4918-af91-a8293f5056db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import os\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "import sklearn\n",
    "import joblib\n",
    "!pip install holidays xgboost lightgbm\n",
    "import holidays\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ca141-d5d7-4259-9bb5-ba8afd698de7",
   "metadata": {},
   "source": [
    "CREATING AN S3 BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef180b-64fb-419f-8ead-2ff8cb1130fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'salesforecastingbucket9321423572'\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490d4d1-e761-44ca-9e13-d09ab4f1c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    print('S3 Bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe30e78-6f1f-46f6-bbf7-fbef7f988d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'salesforecast'\n",
    "output_path = 's3://{}/{}/output'.format(bucket_name,prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937b30a-8b39-4b02-ada8-ebd95e05a4c9",
   "metadata": {},
   "source": [
    "UPLOADING THE DATASET TO THE S3 BUCKET AND LOADING IT IN THE DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c50af-bae6-407a-b220-94ce95061bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da79bc-aee7-41d7-b42d-c9c2aa18faf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f's3://{bucket_name}/{prefix}/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c4015-d0c7-4890-bc26-8f534899a2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5437c-0704-4dc4-8644-1bf7c6854063",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING STEPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e7c3d-1e04-4dfe-846b-6c6fcfd76949",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_map = {\n",
    "    \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\", \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Hawaii\": \"HI\", \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\", \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\", \"Michigan\": \"MI\", \"Minnesota\": \"MN\", \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\", \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\", \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\", \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\", \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\", \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\", \"District of Columbia\": \"DC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b0a0e-ec14-465f-b404-080d5e895629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Order Date'] = pd.to_datetime(df['Order Date'], dayfirst=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980d1d7-b054-463d-88ba-40e90c1485de",
   "metadata": {},
   "source": [
    "MARKING NATIONAL AND STATE HOLIDAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91cfc7-fab3-4a17-a9c0-6f15f4f219a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"State\"] = df[\"State\"].map(state_map).fillna(df[\"State\"])\n",
    "\n",
    "# --- Add is_weekend column ---\n",
    "df[\"is_weekend\"] = df[\"Order Date\"].dt.dayofweek >= 5   # Saturday=5, Sunday=6\n",
    "\n",
    "# --- Add is_holiday column ---\n",
    "def is_us_holiday(date, state_code):\n",
    "    if pd.isna(date):\n",
    "        return False\n",
    "    try:\n",
    "        state_holidays = holidays.US(years=[date.year], state=state_code)\n",
    "    except:\n",
    "        state_holidays = holidays.US(years=[date.year])  # fallback federal only\n",
    "    return date in state_holidays\n",
    "\n",
    "df[\"is_holiday\"] = df.apply(lambda row: is_us_holiday(row[\"Order Date\"], row[\"State\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ed3a1-d4bc-49e2-be0b-1095b2ad3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_sales'] = np.log1p(df['Sales'])\n",
    "df.drop(columns=['Sales'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763ee24-d461-4271-a1de-4356b0eb1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_time_features(df, date_column):\n",
    "    \"\"\"Add comprehensive time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    df['year'] = df[date_column].dt.year\n",
    "    df['month'] = df[date_column].dt.month\n",
    "    df['quarter'] = df[date_column].dt.quarter\n",
    "    df['week_of_year'] = df[date_column].dt.isocalendar().week\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
    "    df['day_of_month'] = df[date_column].dt.day\n",
    "    df['is_month_start'] = df[date_column].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df[date_column].dt.is_month_end.astype(int)\n",
    "    \n",
    "    # Cyclical encoding for periodic features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = enhanced_time_features(df, 'Order Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0f5a2-5b53-4b0d-86fb-f0fabc799605",
   "metadata": {},
   "source": [
    "ENCODING THE COLUMN VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303c241-6514-41e5-add6-8062759c2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "categorical_cols = ['Ship Mode', 'Segment', 'City', 'State', 'Category', 'Sub-Category', 'Region']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].fillna('Unknown').astype(str)\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded {col} with {len(le.classes_)} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca6b7-917c-4433-bd58-f894755bbbdd",
   "metadata": {},
   "source": [
    "DEFINING THE FEATURE AND TARGET COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16d39e-6ccd-42c7-89a6-a3e4e1083c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'year', 'month', 'quarter', 'week_of_year', 'day_of_week', \n",
    "    'day_of_month', 'is_month_start', 'is_month_end',\n",
    "    'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "    'is_weekend', 'is_holiday',\n",
    "    'Ship Mode_encoded', 'Segment_encoded', 'City_encoded', \n",
    "    'State_encoded', 'Category_encoded', 'Sub-Category_encoded', 'Region_encoded'\n",
    "]\n",
    "\n",
    "target_column = 'log_sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84083a60-a2c2-46d9-8d09-b149e1bc3d35",
   "metadata": {},
   "source": [
    "Creating monthly aggregated data with proper holiday/weekend features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff907e-46b6-4c17-a723-b865ff5cf9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating monthly aggregated data with proper holiday/weekend features...\")\n",
    "\n",
    "# First, calculate monthly counts of weekends and holidays\n",
    "monthly_counts = df.groupby([pd.Grouper(key='Order Date', freq='ME'), \n",
    "                           'Region_encoded', 'Category_encoded']).agg({\n",
    "    'log_sales': 'mean',\n",
    "    'is_weekend': 'sum',      # Total weekend days in month\n",
    "    'is_holiday': 'sum',      # Total holiday days in month\n",
    "    'Ship Mode_encoded': 'first',\n",
    "    'Segment_encoded': 'first',\n",
    "    'State_encoded': 'first',\n",
    "    'Sub-Category_encoded': 'first',\n",
    "    'City_encoded': 'first'  # Keep City_encoded for target encoding\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the count columns\n",
    "monthly_counts = monthly_counts.rename(columns={\n",
    "    'is_weekend': 'total_weekends',\n",
    "    'is_holiday': 'total_holidays'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734a6b3-1265-4c7e-a580-faed5579d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = enhanced_time_features(monthly_counts, 'Order Date')\n",
    "\n",
    "# Add proportion features (more meaningful than counts)\n",
    "monthly_data['weekend_proportion'] = monthly_data['total_weekends'] / 8  # Approx max weekends per month\n",
    "monthly_data['holiday_proportion'] = monthly_data['total_holidays'] / 3  # Approx max holidays per month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9add9-0a62-46c1-9158-2f0bcf06c8ff",
   "metadata": {},
   "source": [
    "ROLLING STATISTICS AND GROWTH FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fda90-9394-46db-948d-12a396e3692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding advanced features...\")\n",
    "\n",
    "def add_advanced_features(df):\n",
    "    \"\"\"Add rolling statistics and growth features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by date for rolling calculations\n",
    "    df = df.sort_values(['Region_encoded', 'Category_encoded', 'Order Date'])\n",
    "    \n",
    "    # Rolling statistics (3, 6, 12 month windows)\n",
    "    for window in [3, 6, 12]:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby(['Region_encoded', 'Category_encoded'])['log_sales'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        df[f'rolling_std_{window}'] = df.groupby(['Region_encoded', 'Category_encoded'])['log_sales'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # Year-over-year growth rates\n",
    "    df['yoy_growth'] = df.groupby(['Region_encoded', 'Category_encoded'])['log_sales'].pct_change(12)\n",
    "    \n",
    "    # Month-over-month growth\n",
    "    df['mom_growth'] = df.groupby(['Region_encoded', 'Category_encoded'])['log_sales'].pct_change(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "monthly_data = add_advanced_features(monthly_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9323c-e021-453b-a292-1e10f61982f2",
   "metadata": {},
   "source": [
    "ENCODING FOR HIGH CARDINALITY FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f8462-accf-4dca-ad9a-1300266b1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying manual target encoding for high-cardinality features...\")\n",
    "\n",
    "def manual_target_encoding(df, group_col, target_col, new_col_name):\n",
    "    \"\"\"Manual target encoding implementation\"\"\"\n",
    "    # Calculate mean target by group\n",
    "    group_means = df.groupby(group_col)[target_col].mean().to_dict()\n",
    "    # Apply encoding\n",
    "    df[new_col_name] = df[group_col].map(group_means)\n",
    "    # Fill any NaN values with overall mean\n",
    "    overall_mean = df[target_col].mean()\n",
    "    df[new_col_name] = df[new_col_name].fillna(overall_mean)\n",
    "    return df\n",
    "\n",
    "# Apply target encoding to high-cardinality features\n",
    "monthly_data = manual_target_encoding(monthly_data, 'City_encoded', 'log_sales', 'city_target_encoded')\n",
    "monthly_data = manual_target_encoding(monthly_data, 'State_encoded', 'log_sales', 'state_target_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23bdf8-0245-472a-94fb-c18b968464fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = monthly_data.drop(columns=['City_encoded', 'State_encoded'])\n",
    "\n",
    "print(f\"Monthly data shape: {monthly_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d4894-0a8d-4ca2-b7e0-6c0f83e3efd3",
   "metadata": {},
   "source": [
    "ADDING LAG FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb06a9-404a-4606-9015-9659429c8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df, target_col, lags=[1, 2, 3, 6, 12]):\n",
    "    \"\"\"Add lagged target values as features\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values('Order Date').reset_index(drop=True)\n",
    "    \n",
    "    group_cols = ['Region_encoded', 'Category_encoded']\n",
    "    \n",
    "    for lag in lags:\n",
    "        lag_col = f'{target_col}_lag_{lag}'\n",
    "        df[lag_col] = df.groupby(group_cols)[target_col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "monthly_data_with_lags = add_lag_features(monthly_data, 'log_sales')\n",
    "monthly_data_with_lags = monthly_data_with_lags.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff68f0-7f31-4421-a132-db035066ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update feature list with new features\n",
    "lag_features = [col for col in monthly_data_with_lags.columns if 'lag_' in col]\n",
    "rolling_features = [col for col in monthly_data_with_lags.columns if 'rolling_' in col]\n",
    "growth_features = [col for col in monthly_data_with_lags.columns if 'growth' in col]\n",
    "target_encoded_features = [col for col in monthly_data_with_lags.columns if 'target_encoded' in col]\n",
    "\n",
    "updated_features = [col for col in feature_columns if col in monthly_data_with_lags.columns] + \\\n",
    "                  ['total_weekends', 'total_holidays', 'weekend_proportion', 'holiday_proportion'] + \\\n",
    "                  lag_features + rolling_features + growth_features + target_encoded_features\n",
    "\n",
    "# Remove any duplicate or unnecessary features\n",
    "updated_features = [col for col in updated_features if col in monthly_data_with_lags.columns]\n",
    "updated_features = list(set(updated_features))  # Remove duplicates\n",
    "\n",
    "print(f\"Total features after enhancement: {len(updated_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a466810-f95e-42c4-8f2e-6acd4ebfa690",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbc16f-7c35-403a-bb6a-b0ec33fda19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_time_series_data(data, features, target, test_size=0.2):\n",
    "    \"\"\"Prepare time series data with proper temporal splitting\"\"\"\n",
    "    data = data.sort_values('Order Date').dropna()\n",
    "    \n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    \n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    \n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "    print(f\"Train period: {data['Order Date'].iloc[0]} to {data['Order Date'].iloc[split_idx-1]}\")\n",
    "    print(f\"Test period: {data['Order Date'].iloc[split_idx]} to {data['Order Date'].iloc[-1]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Prepare monthly data\n",
    "X_train_monthly, X_test_monthly, y_train_monthly, y_test_monthly = prepare_time_series_data(\n",
    "    monthly_data_with_lags, updated_features, target_column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be6e5e-291c-4efb-928d-80f4f555e4b3",
   "metadata": {},
   "source": [
    "DEFINING ML MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89f2c1-647d-4e79-8f70-af36accab794",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining ensemble model...\")\n",
    "\n",
    "# Create individual models with optimized parameters\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    min_samples_split=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b9375-2a28-406b-89a8-75ad79fd99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training individual models...\")\n",
    "models = {\n",
    "    'Gradient Boosting': gb_model,\n",
    "    'XGBoost': xgb_model,\n",
    "    'LightGBM': lgbm_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d8a58-e88c-434b-85a4-ad707e6179d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_monthly, y_train_monthly)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    preds_log = model.predict(X_test_monthly)\n",
    "    preds = np.expm1(preds_log)\n",
    "    actuals = np.expm1(y_test_monthly)\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, preds)\n",
    "    mape = np.mean(np.abs((actuals - preds) / actuals)) * 100\n",
    "    \n",
    "    individual_results[name] = {'MAE': mae, 'MAPE': mape}\n",
    "    print(f\"{name}: MAE = ${mae:.2f}, MAPE = {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0713d44-521e-4d0f-957b-d1acfea66b0f",
   "metadata": {},
   "source": [
    "TRAINING ENSEMBLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cbb98-e6a5-40b0-bf86-3467c08cd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining ensemble model...\")\n",
    "ensemble_model = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('gb', gb_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgbm', lgbm_model)\n",
    "    ]\n",
    ")\n",
    "ensemble_model.fit(X_train_monthly, y_train_monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4427f07-56d3-4125-8a3d-3178cc821955",
   "metadata": {},
   "source": [
    "ACCURACY SCORES (MAE, RMSE, MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c1fbf-581f-4925-8c11-aec90fa35a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds_log = ensemble_model.predict(X_test_monthly)\n",
    "ensemble_preds = np.expm1(ensemble_preds_log)\n",
    "ensemble_actuals = np.expm1(y_test_monthly)\n",
    "\n",
    "ensemble_mae = mean_absolute_error(ensemble_actuals, ensemble_preds)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(ensemble_actuals, ensemble_preds))\n",
    "ensemble_mape = np.mean(np.abs((ensemble_actuals - ensemble_preds) / ensemble_actuals)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aacd69-8109-488d-8e6a-c21afe80b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENSEMBLE MODEL PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"MAE: ${ensemble_mae:,.2f}\")\n",
    "print(f\"RMSE: ${ensemble_rmse:,.2f}\")\n",
    "print(f\"MAPE: {ensemble_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ace6c-78b5-4f07-b66b-e8928e038c61",
   "metadata": {},
   "source": [
    "MODEL COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a7ab8-6347-4fe7-b4a2-c612bb8d4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "for name, results in individual_results.items():\n",
    "    print(f\"{name:20} | MAE: ${results['MAE']:,.2f} | MAPE: {results['MAPE']:.2f}%\")\n",
    "\n",
    "print(f\"{'Ensemble':20} | MAE: ${ensemble_mae:,.2f} | MAPE: {ensemble_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f924bdd-b371-42c8-bde0-c37645d03a48",
   "metadata": {},
   "source": [
    "RESIDUAL ANALYSIS AND CORRECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f572e-5017-49fd-a6ba-70d911455079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESIDUAL ANALYSIS AND CORRECTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Calculate residuals\n",
    "train_residuals = y_train_monthly - ensemble_model.predict(X_train_monthly)\n",
    "\n",
    "# Train residual correction model\n",
    "residual_model = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "residual_model.fit(X_train_monthly, train_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb4fb1-711e-4f88-8148-f0b1b8a4ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_preds_log = ensemble_preds_log + residual_model.predict(X_test_monthly)\n",
    "corrected_preds = np.expm1(corrected_preds_log)\n",
    "\n",
    "corrected_mae = mean_absolute_error(ensemble_actuals, corrected_preds)\n",
    "corrected_rmse = np.sqrt(mean_squared_error(ensemble_actuals, corrected_preds))\n",
    "corrected_mape = np.mean(np.abs((ensemble_actuals - corrected_preds) / ensemble_actuals)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f16ab-7384-4fef-81a9-2f05780563ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After residual correction:\")\n",
    "print(f\"MAE: ${corrected_mae:,.2f} (Improvement: {((ensemble_mae - corrected_mae)/ensemble_mae*100):.1f}%)\")\n",
    "print(f\"MAPE: {corrected_mape:.2f}% (Improvement: {((ensemble_mape - corrected_mape)/ensemble_mape*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28372bb5-ba6c-4acb-9371-77979af49fc2",
   "metadata": {},
   "source": [
    "FINAL OPTIMIZED PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a1e79-ff4d-48c6-a1f0-4d019d305b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL OPTIMIZED PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"MAE: ${corrected_mae:,.2f}\")\n",
    "print(f\"RMSE: ${corrected_rmse:,.2f}\")\n",
    "print(f\"MAPE: {corrected_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5be534-dfe1-46c5-bea2-fd489dc103c7",
   "metadata": {},
   "source": [
    "IMPORTANT FEATURES VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc849c5-b3c5-45a1-acf9-9194c2191c71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "        indices = np.argsort(importance)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f\"Feature Importance - {title}\")\n",
    "        plt.barh(range(15), importance[indices][:15][::-1])\n",
    "        plt.yticks(range(15), [feature_names[i] for i in indices[:15]][::-1])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return indices\n",
    "    return None\n",
    "\n",
    "# Plot feature importance for the ensemble's best model\n",
    "feature_indices = plot_feature_importance(gb_model, updated_features, \"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cb983-7e08-40cb-9b27-de2a9b9f4dfb",
   "metadata": {},
   "source": [
    "IMPORTANT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65c6bb-f84b-41a2-bf1d-9493d3292215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if feature_indices is not None:\n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    for i in range(10):\n",
    "        feature_name = updated_features[feature_indices[i]]\n",
    "        importance = gb_model.feature_importances_[feature_indices[i]]\n",
    "        print(f\"  {i+1}. {feature_name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b46e7-f6d8-460e-8ca7-32f2b647e6da",
   "metadata": {},
   "source": [
    "SAVING THE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dff41-4907-4de3-9621-317deed5490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = {\n",
    "    'ensemble': ensemble_model,\n",
    "    'residual_correction': residual_model,\n",
    "    'feature_names': updated_features,\n",
    "    'label_encoders': label_encoders\n",
    "}\n",
    "joblib.dump(final_model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ce61b-04fa-483d-a6c5-a6537be0b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf model.tar.gz model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23161e-93ef-4279-8c54-ea1a190f26cc",
   "metadata": {},
   "source": [
    "UPLOADING THE MODEL TO S3 BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca570aa-a593-46b2-87a6-e0cfa0b6d71d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"salesforecastingbucket9321423572\"\n",
    "s3.upload_file(\"model.tar.gz\", bucket, \"models/model.tar.gz\")\n",
    "\n",
    "# 4. Get final S3 path\n",
    "model_artifact = f\"s3://{bucket}/models/model.tar.gz\"\n",
    "print(\"Uploaded model to:\", model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d36b8-ea0b-4b8d-abcc-b348df0529da",
   "metadata": {},
   "source": [
    "Production prediction function with residual correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2924f9e-1edb-457b-9b8b-f5089b052638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sales(features_df):\n",
    "    \"\"\"Production prediction function with residual correction\"\"\"\n",
    "    # Ensemble prediction\n",
    "    ensemble_pred = final_model['ensemble'].predict(features_df)\n",
    "    # Residual correction\n",
    "    residual_pred = final_model['residual_correction'].predict(features_df)\n",
    "    # Final prediction\n",
    "    final_pred_log = ensemble_pred + residual_pred\n",
    "    return np.expm1(final_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad1440-51f1-44fc-a80d-fcbad1d20b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict_sales(X_test_monthly.iloc[:5])\n",
    "print(f\"\\nSample predictions: {test_predictions}\")\n",
    "print(f\"Actual values: {ensemble_actuals.iloc[:5].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac72cd-a83b-46ef-b25d-c3cdf00eca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la /home/ec2-user/SageMaker/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5699d7f-71aa-4c41-bfc4-fc4a971ac4a1",
   "metadata": {},
   "source": [
    "BUILDING THE DOCKER IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0252f-b77d-48a7-9c7f-371c5b98f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/ec2-user/SageMaker && docker build -t sklearn-170-custom ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321b43d-4cba-4e27-8938-00f627d64fd9",
   "metadata": {},
   "source": [
    "PUSHING THE DOCKER IMAGE TO ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a210a5a-0b42-4d5f-8ec9-928744dc8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.model import Model\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Login to ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "\n",
    "# Create ECR repository (if it doesn't exist)\n",
    "!aws ecr create-repository --repository-name sklearn-170-custom --region {region} || echo \"Repository already exists\"\n",
    "\n",
    "# Tag the Docker image for ECR\n",
    "!docker tag sklearn-170-custom:latest {account_id}.dkr.ecr.{region}.amazonaws.com/sklearn-170-custom:latest\n",
    "\n",
    "# Push the image to ECR\n",
    "!docker push {account_id}.dkr.ecr.{region}.amazonaws.com/sklearn-170-custom:latest\n",
    "\n",
    "print(\"✅ Docker image pushed to ECR successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b78f5-a999-4d15-90a4-d42e603c01db",
   "metadata": {},
   "source": [
    "CREATING AN AWS ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7aca54-1fa9-4570-a7a9-4d504aba7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.model import Model\n",
    "\n",
    "# Your model artifact S3 path (replace with your actual path)\n",
    "model_artifact = model_artifact\n",
    "\n",
    "# Use custom container image\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/sklearn-170-custom:latest\"\n",
    "\n",
    "# Generate unique timestamp for names\n",
    "timestamp = int(time.time())\n",
    "unique_model_name = f\"sklearn-170-custom-model-{timestamp}\"\n",
    "unique_endpoint_name = f\"sklearn-170-custom-endpoint-{timestamp}\"\n",
    "\n",
    "# Create model with custom container\n",
    "custom_model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    name=unique_model_name  # Unique model name\n",
    ")\n",
    "\n",
    "# Deploy endpoint\n",
    "predictor = custom_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=unique_endpoint_name,  # Unique endpoint name\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"✅ Deployment Complete!\")\n",
    "print(f\"Model: {unique_model_name}\")\n",
    "print(f\"Endpoint: {unique_endpoint_name}\")\n",
    "print(f\"Endpoint URL: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef0988-64c8-4b75-8304-9a792970757b",
   "metadata": {},
   "source": [
    "CREATING CUSTOM DOCKER CONTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66faf324-d7d9-480f-a0be-dafb8adbf9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t sales-forecast-model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452f12d-baf6-4929-bf1e-104ae857dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 8080:8080 sales-forecast-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a6f53-f620-48d1-86d5-15622deef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5b097-7d9a-465c-a07a-1ae43884bd7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!curl http://localhost:8080/ping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
